{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Pipeline Testing Notebook\n",
        "\n",
        "This notebook allows testing all components without running Streamlit.\n",
        "\n",
        "**Structure:**\n",
        "1. Setup & Imports\n",
        "2. Test Models (Database)\n",
        "3. Test Tools (Restrictions, Classification)\n",
        "4. Test LLM Service\n",
        "5. Test Full Pipeline (Processor)\n",
        "6. Test RBAC/ABAC (Different Users)\n",
        "7. Test Validation/Guardrails\n",
        "8. Test Scenarios (E2E)\n",
        "\n",
        "**Usage:** Run cells sequentially. Each section is independent after Setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/vladislav/Documents/vlzm/kyc-analyzer/kyc-analyzer\n"
          ]
        }
      ],
      "source": [
        "# Add project root to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Navigate to project root (one level up from notebooks/)\n",
        "# Path().absolute() gets current directory (notebooks/), .parent gets project root\n",
        "PROJECT_ROOT = Path().absolute().parent\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment configured:\n",
            "  ENV: LOCAL\n",
            "  LLM_PROVIDER: openai\n",
            "  DATABASE: app_db @ localhost:5432\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables BEFORE imports\n",
        "import os\n",
        "\n",
        "# Required for local development\n",
        "os.environ.setdefault(\"ENV\", \"LOCAL\")\n",
        "os.environ.setdefault(\"LLM_PROVIDER\", \"openai\")  # or \"openai\", \"azure\", \"anthropic\"\n",
        "os.environ.setdefault(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "os.environ.setdefault(\"OLLAMA_MODEL\", \"llama3.2\")\n",
        "\n",
        "# Database (matches docker-compose.yml - service name is \"db\", not \"postgres\")\n",
        "# Run: docker compose up -d db\n",
        "# NOTE: Using = instead of setdefault() to OVERRIDE .env file values\n",
        "os.environ[\"DATABASE_HOST\"] = \"localhost\"\n",
        "os.environ[\"DATABASE_PORT\"] = \"5432\"\n",
        "os.environ[\"DATABASE_NAME\"] = \"app_db\"  # Matches POSTGRES_DB in docker-compose (overrides .env)\n",
        "os.environ[\"DATABASE_USER\"] = \"postgres\"  # Matches POSTGRES_USER in docker-compose\n",
        "os.environ[\"DATABASE_PASSWORD\"] = \"localdevpassword123\"\n",
        "\n",
        "print(\"Environment configured:\")\n",
        "print(f\"  ENV: {os.environ['ENV']}\")\n",
        "print(f\"  LLM_PROVIDER: {os.environ['LLM_PROVIDER']}\")\n",
        "print(f\"  DATABASE: {os.environ['DATABASE_NAME']} @ {os.environ['DATABASE_HOST']}:{os.environ['DATABASE_PORT']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "from app.models import Request, RequestCreate, AnalysisResult, AnalysisOutput\n",
        "from app.database import init_db, get_session\n",
        "from app.services.processor import Processor\n",
        "from app.services.llm_service import get_llm_service\n",
        "from app.services.auth_mock import get_current_user, UserProfile, Permission, MOCK_USERS, ROLE_PERMISSIONS\n",
        "from app.services.validation import run_all_validations\n",
        "from app.services.tools.definitions import TOOL_DEFINITIONS, TOOL_FUNCTIONS, execute_tool\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Models (Database)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request fields:\n",
            "  id: typing.Optional[int]\n",
            "  input_text: <class 'str'>\n",
            "  context: typing.Optional[str]\n",
            "  group: <class 'str'>\n",
            "  created_by_user_id: typing.Optional[str]\n",
            "  created_at: <class 'datetime.datetime'>\n",
            "\n",
            "AnalysisResult fields:\n",
            "  id: typing.Optional[int]\n",
            "  request_id: <class 'int'>\n",
            "  score: <class 'int'>\n",
            "  categories: list[str]\n",
            "  summary: <class 'str'>\n",
            "  processed_content: typing.Optional[str]\n",
            "  model_version: <class 'str'>\n",
            "  group: <class 'str'>\n",
            "  analyzed_by_user_id: typing.Optional[str]\n",
            "  llm_trace: <class 'dict'>\n",
            "  human_feedback: typing.Optional[bool]\n",
            "  feedback_comment: typing.Optional[str]\n",
            "  feedback_by_user_id: typing.Optional[str]\n",
            "  feedback_at: typing.Optional[datetime.datetime]\n",
            "  validation_status: <class 'str'>\n",
            "  validation_details: typing.Optional[str]\n",
            "  created_at: <class 'datetime.datetime'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/bc/6rm76y3s2f7180l31j70ss7h0000gn/T/ipykernel_55448/571665599.py:3: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use the `model_fields` class property instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  for name, field in Request.__fields__.items():\n",
            "/var/folders/bc/6rm76y3s2f7180l31j70ss7h0000gn/T/ipykernel_55448/571665599.py:7: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use the `model_fields` class property instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  for name, field in AnalysisResult.__fields__.items():\n"
          ]
        }
      ],
      "source": [
        "# Check model fields - verify your schema changes\n",
        "print(\"Request fields:\")\n",
        "for name, field in Request.__fields__.items():\n",
        "    print(f\"  {name}: {field.annotation}\")\n",
        "\n",
        "print(\"\\nAnalysisResult fields:\")\n",
        "for name, field in AnalysisResult.__fields__.items():\n",
        "    print(f\"  {name}: {field.annotation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Database initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize database (creates tables if not exist)\n",
        "init_db()\n",
        "print(\"‚úÖ Database initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Test creating a request manually (with rollback - won't pollute DB)\n",
        "# # This is just for testing the model structure\n",
        "# with get_session() as session:\n",
        "#     test_request = Request(\n",
        "#         input_text=\"Test document content\",\n",
        "#         context=\"Testing from notebook\",\n",
        "#         group=\"test_group\",\n",
        "#     )\n",
        "#     session.add(test_request)\n",
        "#     session.flush()  # Get ID without committing\n",
        "    \n",
        "#     print(f\"‚úÖ Created request with ID: {test_request.id}\")\n",
        "#     print(f\"   Input: {test_request.input_text}\")\n",
        "#     print(f\"   Group: {test_request.group}\")\n",
        "    \n",
        "#     session.rollback()\n",
        "#     print(\"   (rolled back - test only)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved request to DB:\n",
            "   ID: 10\n",
            "   Input: Customer service email from John Smith at ABC Corp regarding account inquiry\n",
            "   Group: default\n"
          ]
        }
      ],
      "source": [
        "# Create a request that WILL be saved to DB (no rollback)\n",
        "with get_session() as session:\n",
        "    saved_request = Request(\n",
        "        input_text=\"Customer service email from John Smith at ABC Corp regarding account inquiry\",\n",
        "        context=\"Test document saved to DB\",\n",
        "        group=\"default\",\n",
        "    )\n",
        "    session.add(saved_request)\n",
        "    # No rollback - this will be committed!\n",
        "    \n",
        "print(f\"‚úÖ Saved request to DB:\")\n",
        "print(f\"   ID: {saved_request.id}\")\n",
        "print(f\"   Input: {saved_request.input_text}\")\n",
        "print(f\"   Group: {saved_request.group}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Found 1 requests in database:\n",
            "\n",
            "  ID: 10\n",
            "    Input: Customer service email from John Smith at ABC Corp regarding...\n",
            "    Group: default\n",
            "    Created: 2026-01-08 11:02:59.158695\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check what's actually in the database\n",
        "from sqlmodel import select\n",
        "\n",
        "with get_session() as session:\n",
        "    # Query all requests\n",
        "    stmt = select(Request)\n",
        "    requests = session.exec(stmt).all()\n",
        "    \n",
        "    print(f\"üìä Found {len(requests)} requests in database:\\n\")\n",
        "    for req in requests:\n",
        "        print(f\"  ID: {req.id}\")\n",
        "        print(f\"    Input: {req.input_text[:60]}...\")\n",
        "        print(f\"    Group: {req.group}\")\n",
        "        print(f\"    Created: {req.created_at}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Tools (Function Calling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered tools: 0\n",
            "‚ÑπÔ∏è No tools defined yet. Add tools in Phase 2.\n",
            "   File: app/services/tools/definitions.py\n",
            "\n",
            "Tool functions available: ['lookup_database', 'validate_data']\n"
          ]
        }
      ],
      "source": [
        "# Check available tools\n",
        "print(f\"Registered tools: {len(TOOL_DEFINITIONS)}\")\n",
        "\n",
        "if TOOL_DEFINITIONS:\n",
        "    for tool in TOOL_DEFINITIONS:\n",
        "        func = tool[\"function\"]\n",
        "        print(f\"\\nüìå {func['name']}\")\n",
        "        print(f\"   Description: {func['description'][:80]}...\")\n",
        "        print(f\"   Parameters: {list(func['parameters']['properties'].keys())}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No tools defined yet. Add tools in Phase 2.\")\n",
        "    print(\"   File: app/services/tools/definitions.py\")\n",
        "\n",
        "print(f\"\\nTool functions available: {list(TOOL_FUNCTIONS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è Uncomment tool tests after implementing tools in Phase 2\n"
          ]
        }
      ],
      "source": [
        "# Test tools directly (uncomment after implementing in Phase 2)\n",
        "# These tests run WITHOUT LLM - just the tool functions\n",
        "\n",
        "# Example: Test restricted entity check\n",
        "# from app.services.tools.restrictions import check_restricted_list\n",
        "# result = check_restricted_list(\"Ahmed Ivanov\")\n",
        "# print(\"Restricted entity check result:\")\n",
        "# print(result)\n",
        "\n",
        "# Example: Test data classification validation  \n",
        "# from app.services.tools.classification import validate_data_classification\n",
        "# result = validate_data_classification(\"confidential\", \"internal\")\n",
        "# print(\"Data classification check result:\")\n",
        "# print(result)\n",
        "\n",
        "print(\"‚ÑπÔ∏è Uncomment tool tests after implementing tools in Phase 2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test LLM Service\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Provider: openai\n",
            "Model: openai/gpt-5.2\n"
          ]
        }
      ],
      "source": [
        "# Get LLM service instance\n",
        "llm_service = get_llm_service()\n",
        "\n",
        "print(f\"LLM Provider: {llm_service.provider.provider_name}\")\n",
        "print(f\"Model: {llm_service.provider.get_model_version()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing simple analysis...\n",
            "Input: Customer support email from John Smith requesting account information update\n",
            "\n",
            "‚úÖ LLM Response:\n",
            "   Score: 18\n",
            "   Categories: ['Customer Support', 'Account Management', 'Information Update Request']\n",
            "   Reasoning: The input describes a customer support email from an individual (John Smith) requesting an account information update. The content is high-level and does not include the actual email body, specific ac...\n"
          ]
        }
      ],
      "source": [
        "# Test simple analysis (no tools)\n",
        "test_input = \"Customer support email from John Smith requesting account information update\"\n",
        "\n",
        "print(f\"Testing simple analysis...\")\n",
        "print(f\"Input: {test_input}\\n\")\n",
        "\n",
        "try:\n",
        "    response = llm_service.analyze(test_input)\n",
        "    print(\"‚úÖ LLM Response:\")\n",
        "    print(f\"   Score: {response.score}\")\n",
        "    print(f\"   Categories: {response.categories}\")\n",
        "    print(f\"   Reasoning: {response.reasoning[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"   Make sure LLM provider is running (ollama, openai key, etc.)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è No tools defined yet. Agent mode test skipped.\n",
            "   Define TOOL_DEFINITIONS in app/services/tools/definitions.py\n"
          ]
        }
      ],
      "source": [
        "# Test analysis WITH tools (agent mode)\n",
        "# Only works if TOOL_DEFINITIONS is not empty\n",
        "\n",
        "if TOOL_DEFINITIONS:\n",
        "    test_input = \"Document contains personal information about Ahmed Ivanov including contact details and identification numbers\"\n",
        "    \n",
        "    print(f\"Testing agent mode with tools...\")\n",
        "    print(f\"Input: {test_input}\\n\")\n",
        "    \n",
        "    try:\n",
        "        response = llm_service.analyze_with_tools(test_input)\n",
        "        print(\"‚úÖ Agent Response:\")\n",
        "        print(f\"   Score: {response.score}\")\n",
        "        print(f\"   Categories: {response.categories}\")\n",
        "        print(f\"   Tools used: {response.tools_used}\")\n",
        "        print(f\"   Reasoning: {response.reasoning[:300]}...\")\n",
        "        \n",
        "        if response.trace:\n",
        "            print(f\"\\n   Trace keys: {list(response.trace.keys())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No tools defined yet. Agent mode test skipped.\")\n",
        "    print(\"   Define TOOL_DEFINITIONS in app/services/tools/definitions.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Full Pipeline (Processor)\n",
        "\n",
        "This is what Streamlit does behind the scenes - the complete analysis flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing as user: usr_003 (role: UserRole.ANALYST)\n",
            "  Permissions: ['view', 'view_sensitive', 'analyze']\n",
            "  Group: group_a\n"
          ]
        }
      ],
      "source": [
        "# Get a test user for RBAC\n",
        "user = get_current_user(\"analyst_a\")\n",
        "print(f\"Testing as user: {user.id} (role: {user.role})\")\n",
        "print(f\"  Permissions: {[p.value for p in ROLE_PERMISSIONS.get(user.role, set())]}\")\n",
        "print(f\"  Group: {user.group.value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing request...\n",
            "  Input: Internal document contains sensitive personal information about Elena Volkova including full address, phone number, and date of birth\n",
            "  Context: High-risk data exposure scenario\n",
            "\n",
            "‚úÖ Pipeline completed!\n",
            "\n",
            "üìã Request (ID: 11)\n",
            "   Input: Internal document contains sensitive personal information about Elena Volkova in...\n",
            "   Group: group_a\n",
            "\n",
            "üìä Analysis Result (ID: 6)\n",
            "   Score: 92\n",
            "   Categories: ['Sensitive Personal Data (PII)', 'Data Exposure Risk', 'Privacy Compliance', 'Internal Document Handling', 'Security Incident Potential']\n",
            "   Summary: The input indicates an internal document contains sensitive personally identifiable information (PII) about an individual (Elena Volkova), specifically full address, phone number, and date of birth. T...\n",
            "   Model: openai/gpt-5.2\n",
            "   Validation: PASS\n",
            "\n",
            "üîç LLM Trace:\n",
            "   Keys: ['started_at', 'model', 'mode', 'input', 'completed_at', 'raw_response_preview']\n"
          ]
        }
      ],
      "source": [
        "# Run full analysis pipeline\n",
        "test_data = RequestCreate(\n",
        "    input_text=\"Internal document contains sensitive personal information about Elena Volkova including full address, phone number, and date of birth\",\n",
        "    context=\"High-risk data exposure scenario\",\n",
        "    group=\"default\",\n",
        ")\n",
        "\n",
        "print(f\"Processing request...\")\n",
        "print(f\"  Input: {test_data.input_text}\")\n",
        "print(f\"  Context: {test_data.context}\\n\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor = Processor(session, user=user)\n",
        "    \n",
        "    try:\n",
        "        request, result = processor.process_request(test_data)\n",
        "        \n",
        "        print(\"‚úÖ Pipeline completed!\")\n",
        "        print(f\"\\nüìã Request (ID: {request.id})\")\n",
        "        print(f\"   Input: {request.input_text[:80]}...\")\n",
        "        print(f\"   Group: {request.group}\")\n",
        "        \n",
        "        print(f\"\\nüìä Analysis Result (ID: {result.id})\")\n",
        "        print(f\"   Score: {result.score}\")\n",
        "        print(f\"   Categories: {result.categories}\")\n",
        "        print(f\"   Summary: {result.summary[:200]}...\")\n",
        "        print(f\"   Model: {result.model_version}\")\n",
        "        print(f\"   Validation: {result.validation_status}\")\n",
        "        \n",
        "        if result.llm_trace:\n",
        "            print(f\"\\nüîç LLM Trace:\")\n",
        "            print(f\"   Keys: {list(result.llm_trace.keys())}\")\n",
        "            if \"tools_called\" in result.llm_trace:\n",
        "                print(f\"   Tools called: {result.llm_trace['tools_called']}\")\n",
        "                \n",
        "    except PermissionError as e:\n",
        "        print(f\"‚ùå Permission denied: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'simple'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.llm_trace['mode']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test RBAC/ABAC (Different Users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'admin_default': UserProfile(id='usr_001', username='Alice Administrator', email='alice.admin@example.com', role=<UserRole.ADMIN: 'admin'>, group=<Group.DEFAULT: 'default'>),\n",
              " 'senior_default': UserProfile(id='usr_002', username='Bob Senior Analyst', email='bob.senior@example.com', role=<UserRole.SENIOR_ANALYST: 'senior_analyst'>, group=<Group.DEFAULT: 'default'>),\n",
              " 'analyst_a': UserProfile(id='usr_003', username='Carol Analyst (Group A)', email='carol.analyst@example.com', role=<UserRole.ANALYST: 'analyst'>, group=<Group.GROUP_A: 'group_a'>),\n",
              " 'analyst_b': UserProfile(id='usr_004', username='David Analyst (Group B)', email='david.analyst@example.com', role=<UserRole.ANALYST: 'analyst'>, group=<Group.GROUP_B: 'group_b'>),\n",
              " 'viewer_a': UserProfile(id='usr_005', username='Eve Viewer (Group A)', email='eve.viewer@example.com', role=<UserRole.VIEWER: 'viewer'>, group=<Group.GROUP_A: 'group_a'>)}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MOCK_USERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available mock users:\n",
            "\n",
            "  admin_default:\n",
            "    Role: UserRole.ADMIN\n",
            "  Permissions: ['analyze', 'view_sensitive', 'export_data', 'view_all_groups', 'manage_users', 'view']\n",
            "    Groups: Group.DEFAULT\n",
            "\n",
            "  senior_default:\n",
            "    Role: UserRole.SENIOR_ANALYST\n",
            "  Permissions: ['analyze', 'view_sensitive', 'export_data', 'view_all_groups', 'view']\n",
            "    Groups: Group.DEFAULT\n",
            "\n",
            "  analyst_a:\n",
            "    Role: UserRole.ANALYST\n",
            "  Permissions: ['analyze', 'view_sensitive', 'view']\n",
            "    Groups: Group.GROUP_A\n",
            "\n",
            "  analyst_b:\n",
            "    Role: UserRole.ANALYST\n",
            "  Permissions: ['analyze', 'view_sensitive', 'view']\n",
            "    Groups: Group.GROUP_B\n",
            "\n",
            "  viewer_a:\n",
            "    Role: UserRole.VIEWER\n",
            "  Permissions: ['view']\n",
            "    Groups: Group.GROUP_A\n"
          ]
        }
      ],
      "source": [
        "# List all available mock users\n",
        "print(\"Available mock users:\")\n",
        "for user_key, user in MOCK_USERS.items():\n",
        "    print(f\"\\n  {user_key}:\")\n",
        "    print(f\"    Role: {user.role}\")\n",
        "    print(f\"  Permissions: {[p.value for p in ROLE_PERMISSIONS.get(user.role, set())]}\")\n",
        "    print(f\"    Groups: {user.group}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "set[app.services.auth_mock.Permission]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from app.services.auth_mock import Permission\n",
        "set[Permission]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing as VIEWER: usr_005\n",
            "  Permissions: ['view', 'view_sensitive', 'analyze']\n",
            "‚úÖ Correctly blocked: Access denied. User 'Eve Viewer (Group A)' with role 'viewer' does not have permission 'analyze'.\n"
          ]
        }
      ],
      "source": [
        "# Test RBAC - viewer should NOT be able to analyze\n",
        "from app.services.auth_mock import Permission\n",
        "\n",
        "\n",
        "viewer = get_current_user(\"viewer_a\")\n",
        "print(f\"Testing as VIEWER: {viewer.id}\")\n",
        "print(f\"  Permissions: {[p.value for p in ROLE_PERMISSIONS.get(user.role, set[Permission]())]}\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor = Processor(session, user=viewer)\n",
        "    \n",
        "    try:\n",
        "        request, result = processor.process_request(RequestCreate(\n",
        "            input_text=\"Test document content\",\n",
        "        ))\n",
        "        print(\"‚ùå Should have failed! Viewer shouldn't be able to analyze.\")\n",
        "    except PermissionError as e:\n",
        "        print(f\"‚úÖ Correctly blocked: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyst A groups: Group.GROUP_A\n",
            "Analyst B groups: Group.GROUP_B\n",
            "\n",
            "Analyst A sees 1 results\n",
            "Analyst B sees 0 results\n",
            "Analyst A result groups: {'group_a'}\n"
          ]
        }
      ],
      "source": [
        "# Test ABAC - analysts only see their group's data\n",
        "analyst_a = get_current_user(\"analyst_a\")\n",
        "analyst_b = get_current_user(\"analyst_b\")\n",
        "\n",
        "print(f\"Analyst A groups: {analyst_a.group}\")\n",
        "print(f\"Analyst B groups: {analyst_b.group}\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor_a = Processor(session, user=analyst_a)\n",
        "    results_a = processor_a.get_recent_results(limit=10)\n",
        "    \n",
        "    processor_b = Processor(session, user=analyst_b)\n",
        "    results_b = processor_b.get_recent_results(limit=10)\n",
        "    \n",
        "    print(f\"\\nAnalyst A sees {len(results_a)} results\")\n",
        "    print(f\"Analyst B sees {len(results_b)} results\")\n",
        "    \n",
        "    if results_a:\n",
        "        print(f\"Analyst A result groups: {set(r.group for r in results_a)}\")\n",
        "    if results_b:\n",
        "        print(f\"Analyst B result groups: {set(r.group for r in results_b)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Validation / Guardrails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation status: PASS\n",
            "Details: None\n",
            "Passed: True\n"
          ]
        }
      ],
      "source": [
        "# Test validation functions directly\n",
        "from app.services.llm_service import LLMResponse\n",
        "\n",
        "# Create mock LLM response WITH potential PII leakage\n",
        "mock_response = LLMResponse(\n",
        "    score=75,\n",
        "    categories=[\"sensitive_data\", \"high_risk\"],\n",
        "    reasoning=\"This document contains sensitive personal information. The individual's SSN is 123-45-6789.\",\n",
        ")\n",
        "\n",
        "original_input = \"Document from John Smith\"\n",
        "\n",
        "result = run_all_validations(\n",
        "    response_text=mock_response.reasoning,\n",
        "    score=mock_response.score,              \n",
        "    categories=mock_response.categories\n",
        ")\n",
        "\n",
        "print(f\"Validation status: {result.status}\")\n",
        "print(f\"Details: {result.details}\")\n",
        "print(f\"Passed: {result.passed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Scenarios (E2E)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defined 4 test scenarios:\n",
            "  1. Clean Document - Expected: LOW\n",
            "  2. Partial PII Exposure - Expected: MEDIUM\n",
            "  3. Restricted Entity Match - Expected: CRITICAL\n",
            "  4. Sensitive Person Data - Expected: HIGH\n"
          ]
        }
      ],
      "source": [
        "# Define test scenarios for content analysis\n",
        "TEST_SCENARIOS = [\n",
        "    {\n",
        "        \"name\": \"Clean Document\",\n",
        "        \"input\": \"General customer service email from ABC Corp regarding product inquiry\",\n",
        "        \"expected_risk\": \"LOW\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Partial PII Exposure\",\n",
        "        \"input\": \"Customer support ticket contains name and email address for monthly subscription inquiry\",\n",
        "        \"expected_risk\": \"MEDIUM\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Restricted Entity Match\",\n",
        "        \"input\": \"Document contains personal information about Ahmed Ivanov including full contact details and identification numbers\",\n",
        "        \"expected_risk\": \"CRITICAL\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Sensitive Person Data\",\n",
        "        \"input\": \"Internal document includes comprehensive personal information about Elena Volkova: full address, phone number, date of birth, and employment details\",\n",
        "        \"expected_risk\": \"HIGH\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(TEST_SCENARIOS)} test scenarios:\")\n",
        "for i, scenario in enumerate(TEST_SCENARIOS, 1):\n",
        "    print(f\"  {i}. {scenario['name']} - Expected: {scenario['expected_risk']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "RAG uses pgvector to find similar historical cases based on semantic similarity.\n",
        "This feature can be disabled with `RAG_ENABLED=false`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if RAG is enabled and available\n",
        "from app.services.rag_service import RAGService, get_rag_service\n",
        "from app.services.secret_manager import get_settings\n",
        "\n",
        "settings = get_settings()\n",
        "print(f\"RAG Enabled: {settings.rag_enabled}\")\n",
        "print(f\"Embedding Model: {settings.embedding_model}\")\n",
        "print(f\"Embedding Dimensions: {settings.embedding_dimensions}\")\n",
        "\n",
        "# Check pgvector availability\n",
        "from app.models import PGVECTOR_AVAILABLE\n",
        "print(f\"pgvector Available: {PGVECTOR_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test embedding generation directly\n",
        "with get_session() as session:\n",
        "    rag_service = get_rag_service(session)\n",
        "    \n",
        "    if rag_service.is_enabled:\n",
        "        test_text = \"Customer support email requesting account information update\"\n",
        "        \n",
        "        print(f\"Testing embedding generation...\")\n",
        "        print(f\"Input: {test_text}\\n\")\n",
        "        \n",
        "        try:\n",
        "            embedding = rag_service.get_embedding(test_text)\n",
        "            print(f\"‚úÖ Embedding generated successfully!\")\n",
        "            print(f\"   Dimensions: {len(embedding)}\")\n",
        "            print(f\"   First 5 values: {embedding[:5]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generating embedding: {e}\")\n",
        "            print(\"   Make sure OPENAI_API_KEY is set in your environment\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è RAG is disabled. Set RAG_ENABLED=true to test embeddings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test full pipeline with embedding generation\n",
        "# This creates analysis results WITH embeddings for similarity search\n",
        "\n",
        "user = get_current_user(\"analyst_a\")\n",
        "\n",
        "# Create several test cases for RAG to search through\n",
        "test_cases = [\n",
        "    {\n",
        "        \"input\": \"Customer John Smith requested password reset for account A12345\",\n",
        "        \"context\": \"Password reset scenario\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Suspicious login attempt detected for user account from unknown IP address\",\n",
        "        \"context\": \"Security alert scenario\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Internal memo regarding employee salary information and benefits\",\n",
        "        \"context\": \"HR document scenario\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"Creating test cases with embeddings...\\n\")\n",
        "\n",
        "created_results = []\n",
        "with get_session() as session:\n",
        "    processor = Processor(session, user=user)\n",
        "    \n",
        "    # Check if RAG is enabled\n",
        "    print(f\"RAG enabled: {processor.is_rag_enabled()}\\n\")\n",
        "    \n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"Processing case {i}: {case['input'][:50]}...\")\n",
        "        \n",
        "        try:\n",
        "            request, result = processor.process_request(RequestCreate(\n",
        "                input_text=case[\"input\"],\n",
        "                context=case[\"context\"],\n",
        "            ))\n",
        "            \n",
        "            created_results.append(result)\n",
        "            \n",
        "            # Check if embedding was created\n",
        "            has_embedding = result.embedding is not None and len(result.embedding) > 0\n",
        "            print(f\"   ‚úÖ Result {result.id} created, Score: {result.score}, Has embedding: {has_embedding}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(created_results)} results for similarity testing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test similarity search\n",
        "# Search for cases similar to a new query\n",
        "\n",
        "test_query = \"User requested account password change due to forgotten credentials\"\n",
        "\n",
        "print(f\"üîç Searching for similar cases...\")\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "with get_session() as session:\n",
        "    rag_service = get_rag_service(session)\n",
        "    \n",
        "    if rag_service.is_enabled:\n",
        "        try:\n",
        "            similar_cases = rag_service.find_similar_cases(\n",
        "                query_text=test_query,\n",
        "                limit=3,\n",
        "            )\n",
        "            \n",
        "            if similar_cases:\n",
        "                print(f\"‚úÖ Found {len(similar_cases)} similar cases:\\n\")\n",
        "                for i, case in enumerate(similar_cases, 1):\n",
        "                    print(f\"  {i}. Result ID: {case.id}\")\n",
        "                    print(f\"     Score: {case.score}\")\n",
        "                    print(f\"     Categories: {case.categories}\")\n",
        "                    print(f\"     Summary: {case.summary[:100]}...\")\n",
        "                    print()\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è No similar cases found (database may be empty or no embeddings)\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during similarity search: {e}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è RAG is disabled. Set RAG_ENABLED=true to test similarity search.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test finding similar cases through Processor (with ABAC filtering)\n",
        "# This demonstrates how the UI/API would use similar case search\n",
        "\n",
        "user = get_current_user(\"analyst_a\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor = Processor(session, user=user)\n",
        "    \n",
        "    # First, get the most recent result\n",
        "    recent_results = processor.get_recent_results(limit=1)\n",
        "    \n",
        "    if recent_results:\n",
        "        current_result = recent_results[0]\n",
        "        print(f\"Finding cases similar to Result {current_result.id}:\")\n",
        "        print(f\"   Score: {current_result.score}\")\n",
        "        print(f\"   Categories: {current_result.categories}\\n\")\n",
        "        \n",
        "        if processor.is_rag_enabled():\n",
        "            similar = processor.find_similar_cases(current_result, limit=3)\n",
        "            \n",
        "            if similar:\n",
        "                print(f\"‚úÖ Found {len(similar)} similar cases:\\n\")\n",
        "                for i, case in enumerate(similar, 1):\n",
        "                    print(f\"  {i}. Result ID: {case.id}\")\n",
        "                    print(f\"     Score: {case.score}\")\n",
        "                    print(f\"     Summary: {case.summary[:80]}...\")\n",
        "                    print()\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è No similar cases found\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è RAG is disabled\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No results in database to test with\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running test scenarios...\n",
            "\n",
            "‚úÖ Clean Document\n",
            "   Expected: LOW, Got: LOW (score: 5)\n",
            "   Categories: ['Customer Service', 'Product Inquiry', 'Business Communication', 'Low Risk / Clean Content']\n",
            "   Validation: PASS\n",
            "\n",
            "‚úÖ Partial PII Exposure\n",
            "   Expected: MEDIUM, Got: MEDIUM (score: 45)\n",
            "   Categories: ['PII', 'Customer Support', 'Subscription/Billing Inquiry', 'Data Privacy']\n",
            "   Validation: PASS\n",
            "\n",
            "‚úÖ Restricted Entity Match\n",
            "   Expected: CRITICAL, Got: CRITICAL (score: 82)\n",
            "   Categories: ['Personal Data / PII', 'Sensitive Identifiers', 'Privacy & Data Protection', 'Compliance Screening (Restricted Entity Match)', 'Data Handling / Access Control']\n",
            "   Validation: PASS\n",
            "\n",
            "‚ö†Ô∏è Sensitive Person Data\n",
            "   Expected: HIGH, Got: CRITICAL (score: 92)\n",
            "   Categories: ['Personal Data (PII)', 'Sensitive Personal Information', 'Privacy/Confidentiality Risk', 'Internal Document Handling', 'Data Protection/Compliance']\n",
            "   Validation: PASS\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run all test scenarios\n",
        "def run_test_scenario(scenario: dict, user: UserProfile):\n",
        "    \"\"\"Run a single test scenario and return results.\"\"\"\n",
        "    with get_session() as session:\n",
        "        processor = Processor(session, user=user)\n",
        "        \n",
        "        request_data = RequestCreate(\n",
        "            input_text=scenario[\"input\"],\n",
        "            context=f\"Test: {scenario['name']}\",\n",
        "        )\n",
        "        \n",
        "        request, result = processor.process_request(request_data)\n",
        "        \n",
        "        return {\n",
        "            \"name\": scenario[\"name\"],\n",
        "            \"expected\": scenario[\"expected_risk\"],\n",
        "            \"actual_score\": result.score,\n",
        "            \"categories\": result.categories,\n",
        "            \"validation\": result.validation_status,\n",
        "            \"summary\": result.summary[:100] + \"...\",\n",
        "        }\n",
        "\n",
        "# Run scenarios\n",
        "user = get_current_user(\"analyst_a\")\n",
        "\n",
        "print(\"Running test scenarios...\\n\")\n",
        "for scenario in TEST_SCENARIOS:\n",
        "    try:\n",
        "        result = run_test_scenario(scenario, user)\n",
        "        \n",
        "        # Determine risk level from score\n",
        "        score = result[\"actual_score\"]\n",
        "        if score <= 25:\n",
        "            actual_level = \"LOW\"\n",
        "        elif score <= 50:\n",
        "            actual_level = \"MEDIUM\"\n",
        "        elif score <= 75:\n",
        "            actual_level = \"HIGH\"\n",
        "        else:\n",
        "            actual_level = \"CRITICAL\"\n",
        "        \n",
        "        match = \"‚úÖ\" if actual_level == result[\"expected\"] else \"‚ö†Ô∏è\"\n",
        "        \n",
        "        print(f\"{match} {result['name']}\")\n",
        "        print(f\"   Expected: {result['expected']}, Got: {actual_level} (score: {score})\")\n",
        "        print(f\"   Categories: {result['categories']}\")\n",
        "        print(f\"   Validation: {result['validation']}\")\n",
        "        print()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {scenario['name']}: {e}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Debug Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last 5 analysis results:\n",
            "\n",
            "ID: 10 | Score: 92 | Status: PASS\n",
            "   Categories: ['Personal Data (PII)', 'Sensitive Personal Information', 'Privacy/Confidentiality Risk', 'Internal Document Handling', 'Data Protection/Compliance']\n",
            "   Created: 2026-01-08 11:05:23.550157\n",
            "\n",
            "ID: 9 | Score: 82 | Status: PASS\n",
            "   Categories: ['Personal Data / PII', 'Sensitive Identifiers', 'Privacy & Data Protection', 'Compliance Screening (Restricted Entity Match)', 'Data Handling / Access Control']\n",
            "   Created: 2026-01-08 11:05:17.936367\n",
            "\n",
            "ID: 8 | Score: 45 | Status: PASS\n",
            "   Categories: ['PII', 'Customer Support', 'Subscription/Billing Inquiry', 'Data Privacy']\n",
            "   Created: 2026-01-08 11:05:10.915191\n",
            "\n",
            "ID: 7 | Score: 5 | Status: PASS\n",
            "   Categories: ['Customer Service', 'Product Inquiry', 'Business Communication', 'Low Risk / Clean Content']\n",
            "   Created: 2026-01-08 11:05:06.452610\n",
            "\n",
            "ID: 6 | Score: 92 | Status: PASS\n",
            "   Categories: ['Sensitive Personal Data (PII)', 'Data Exposure Risk', 'Privacy Compliance', 'Internal Document Handling', 'Security Incident Potential']\n",
            "   Created: 2026-01-08 11:03:59.356107\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Helper: View recent results from DB\n",
        "from sqlmodel import select\n",
        "\n",
        "with get_session() as session:\n",
        "    stmt = select(AnalysisResult).order_by(AnalysisResult.created_at.desc()).limit(5)\n",
        "    results = session.exec(stmt).all()\n",
        "    \n",
        "    print(f\"Last {len(results)} analysis results:\\n\")\n",
        "    for r in results:\n",
        "        print(f\"ID: {r.id} | Score: {r.score} | Status: {r.validation_status}\")\n",
        "        print(f\"   Categories: {r.categories}\")\n",
        "        print(f\"   Created: {r.created_at}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No trace found for result 1\n"
          ]
        }
      ],
      "source": [
        "# Helper: View LLM trace for a specific result\n",
        "import json\n",
        "\n",
        "result_id = 1  # Change this to inspect different results\n",
        "\n",
        "with get_session() as session:\n",
        "    result = session.get(AnalysisResult, result_id)\n",
        "    \n",
        "    if result and result.llm_trace:\n",
        "        print(f\"LLM Trace for result {result_id}:\")\n",
        "        print(json.dumps(result.llm_trace, indent=2, default=str))\n",
        "    else:\n",
        "        print(f\"No trace found for result {result_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All test data cleared\n"
          ]
        }
      ],
      "source": [
        "# Helper: Clear all test data (use carefully!)\n",
        "# Uncomment to run\n",
        "\n",
        "from sqlmodel import text\n",
        "with get_session() as session:\n",
        "    session.exec(text(\"DELETE FROM analysis_results\"))\n",
        "    session.exec(text(\"DELETE FROM requests\"))\n",
        "    print(\"‚úÖ All test data cleared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Quick Reference\n",
        "\n",
        "### Before Interview:\n",
        "1. `docker compose up -d db` - Start database with pgvector (service name is **db**)\n",
        "2. Check LLM provider (Ollama running OR API key set)\n",
        "3. Run Setup cells (1.1 - 1.3)\n",
        "\n",
        "### During Interview - Quick Validation:\n",
        "| Phase | Test Section |\n",
        "|-------|-------------|\n",
        "| Phase 1 (Models) | Section 2 |\n",
        "| Phase 2 (Tools) | Section 3 |\n",
        "| Phase 3 (Prompts) | Sections 4-5 |\n",
        "| Phase 4 (Validation) | Section 7 |\n",
        "| Phase 5 (Processor) | Section 5 |\n",
        "| Phase 6 (UI) | Streamlit browser |\n",
        "| RAG (Vector Search) | Section 9 |\n",
        "\n",
        "### Hotkeys:\n",
        "- `Shift+Enter` - Run cell and move to next\n",
        "- `Ctrl+Enter` - Run cell and stay\n",
        "- `Esc + A` - Insert cell above\n",
        "- `Esc + B` - Insert cell below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
