{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Pipeline Testing Notebook\n",
        "\n",
        "This notebook allows testing all components without running Streamlit.\n",
        "\n",
        "**Structure:**\n",
        "1. Setup & Imports\n",
        "2. Test Models (Database)\n",
        "3. Test Tools (Sanctions, Thresholds)\n",
        "4. Test LLM Service\n",
        "5. Test Full Pipeline (Processor)\n",
        "6. Test RBAC/ABAC (Different Users)\n",
        "7. Test Validation/Guardrails\n",
        "8. Test Scenarios (E2E)\n",
        "\n",
        "**Usage:** Run cells sequentially. Each section is independent after Setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add project root to path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Navigate to project root (one level up from notebooks/)\n",
        "PROJECT_ROOT = Path().absolute().parent\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables BEFORE imports\n",
        "import os\n",
        "\n",
        "# Required for local development\n",
        "os.environ.setdefault(\"ENV\", \"LOCAL\")\n",
        "os.environ.setdefault(\"LLM_PROVIDER\", \"ollama\")  # or \"openai\", \"azure\", \"anthropic\"\n",
        "os.environ.setdefault(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "os.environ.setdefault(\"OLLAMA_MODEL\", \"llama3.2\")\n",
        "\n",
        "# Database (use docker compose postgres)\n",
        "os.environ.setdefault(\"DATABASE_HOST\", \"localhost\")\n",
        "os.environ.setdefault(\"DATABASE_PORT\", \"5432\")\n",
        "os.environ.setdefault(\"DATABASE_NAME\", \"genai_db\")\n",
        "os.environ.setdefault(\"DATABASE_USER\", \"genai_user\")\n",
        "os.environ.setdefault(\"DATABASE_PASSWORD\", \"localdevpassword123\")\n",
        "\n",
        "print(\"Environment configured:\")\n",
        "print(f\"  ENV: {os.environ['ENV']}\")\n",
        "print(f\"  LLM_PROVIDER: {os.environ['LLM_PROVIDER']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from app.models import Request, RequestCreate, AnalysisResult, AnalysisOutput\n",
        "from app.database import init_db, get_session\n",
        "from app.services.processor import Processor\n",
        "from app.services.llm_service import get_llm_service\n",
        "from app.services.auth_mock import get_current_user, UserProfile, Permission, MOCK_USERS\n",
        "from app.services.validation import run_all_validations\n",
        "from app.services.tools.definitions import TOOL_DEFINITIONS, TOOL_FUNCTIONS, execute_tool\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Models (Database)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check model fields - verify your schema changes\n",
        "print(\"Request fields:\")\n",
        "for name, field in Request.__fields__.items():\n",
        "    print(f\"  {name}: {field.annotation}\")\n",
        "\n",
        "print(\"\\nAnalysisResult fields:\")\n",
        "for name, field in AnalysisResult.__fields__.items():\n",
        "    print(f\"  {name}: {field.annotation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize database (creates tables if not exist)\n",
        "init_db()\n",
        "print(\"‚úÖ Database initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test creating a request manually (with rollback - won't pollute DB)\n",
        "with get_session() as session:\n",
        "    test_request = Request(\n",
        "        input_text=\"Test transaction comment\",\n",
        "        context=\"Testing from notebook\",\n",
        "        group=\"test_group\",\n",
        "    )\n",
        "    session.add(test_request)\n",
        "    session.flush()  # Get ID without committing\n",
        "    \n",
        "    print(f\"‚úÖ Created request with ID: {test_request.id}\")\n",
        "    print(f\"   Input: {test_request.input_text}\")\n",
        "    print(f\"   Group: {test_request.group}\")\n",
        "    \n",
        "    session.rollback()\n",
        "    print(\"   (rolled back - test only)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Tools (Function Calling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available tools\n",
        "print(f\"Registered tools: {len(TOOL_DEFINITIONS)}\")\n",
        "\n",
        "if TOOL_DEFINITIONS:\n",
        "    for tool in TOOL_DEFINITIONS:\n",
        "        func = tool[\"function\"]\n",
        "        print(f\"\\nüìå {func['name']}\")\n",
        "        print(f\"   Description: {func['description'][:80]}...\")\n",
        "        print(f\"   Parameters: {list(func['parameters']['properties'].keys())}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No tools defined yet. Add tools in Phase 2.\")\n",
        "    print(\"   File: app/services/tools/definitions.py\")\n",
        "\n",
        "print(f\"\\nTool functions available: {list(TOOL_FUNCTIONS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test tools directly (uncomment after implementing in Phase 2)\n",
        "# These tests run WITHOUT LLM - just the tool functions\n",
        "\n",
        "# Example: Test sanctions check\n",
        "# from app.services.tools.sanctions import check_sanctions_list\n",
        "# result = check_sanctions_list(\"Ahmed Ivanov\")\n",
        "# print(\"Sanctions check result:\")\n",
        "# print(result)\n",
        "\n",
        "# Example: Test threshold validation  \n",
        "# from app.services.tools.thresholds import validate_amount_threshold\n",
        "# result = validate_amount_threshold(9500, \"USD\")\n",
        "# print(\"Threshold check result:\")\n",
        "# print(result)\n",
        "\n",
        "print(\"‚ÑπÔ∏è Uncomment tool tests after implementing tools in Phase 2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test LLM Service\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get LLM service instance\n",
        "llm_service = get_llm_service()\n",
        "\n",
        "print(f\"LLM Provider: {llm_service.provider.provider_name}\")\n",
        "print(f\"Model: {llm_service.provider.get_model_version()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test simple analysis (no tools)\n",
        "test_input = \"Payment for consulting services from John Smith, amount $5000\"\n",
        "\n",
        "print(f\"Testing simple analysis...\")\n",
        "print(f\"Input: {test_input}\\n\")\n",
        "\n",
        "try:\n",
        "    response = llm_service.analyze(test_input)\n",
        "    print(\"‚úÖ LLM Response:\")\n",
        "    print(f\"   Score: {response.score}\")\n",
        "    print(f\"   Categories: {response.categories}\")\n",
        "    print(f\"   Summary: {response.summary[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"   Make sure LLM provider is running (ollama, openai key, etc.)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test analysis WITH tools (agent mode)\n",
        "# Only works if TOOL_DEFINITIONS is not empty\n",
        "\n",
        "if TOOL_DEFINITIONS:\n",
        "    test_input = \"Wire transfer from Ahmed Ivanov for $9500 USD\"\n",
        "    \n",
        "    print(f\"Testing agent mode with tools...\")\n",
        "    print(f\"Input: {test_input}\\n\")\n",
        "    \n",
        "    try:\n",
        "        response = llm_service.analyze_with_tools(test_input)\n",
        "        print(\"‚úÖ Agent Response:\")\n",
        "        print(f\"   Score: {response.score}\")\n",
        "        print(f\"   Categories: {response.categories}\")\n",
        "        print(f\"   Tools used: {response.tools_used}\")\n",
        "        print(f\"   Summary: {response.summary[:300]}...\")\n",
        "        \n",
        "        if response.trace:\n",
        "            print(f\"\\n   Trace keys: {list(response.trace.keys())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No tools defined yet. Agent mode test skipped.\")\n",
        "    print(\"   Define TOOL_DEFINITIONS in app/services/tools/definitions.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Full Pipeline (Processor)\n",
        "\n",
        "This is what Streamlit does behind the scenes - the complete analysis flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a test user for RBAC\n",
        "user = get_current_user(\"analyst_a\")\n",
        "print(f\"Testing as user: {user.user_id} (role: {user.role})\")\n",
        "print(f\"  Permissions: {[p.value for p in user.permissions]}\")\n",
        "print(f\"  Groups: {user.groups}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run full analysis pipeline\n",
        "test_data = RequestCreate(\n",
        "    input_text=\"International wire transfer from Elena Volkova for real estate purchase, amount $150,000\",\n",
        "    context=\"High-value cross-border transaction\",\n",
        "    group=\"default\",\n",
        ")\n",
        "\n",
        "print(f\"Processing request...\")\n",
        "print(f\"  Input: {test_data.input_text}\")\n",
        "print(f\"  Context: {test_data.context}\\n\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor = Processor(session, user=user)\n",
        "    \n",
        "    try:\n",
        "        request, result = processor.process_request(test_data)\n",
        "        \n",
        "        print(\"‚úÖ Pipeline completed!\")\n",
        "        print(f\"\\nüìã Request (ID: {request.id})\")\n",
        "        print(f\"   Input: {request.input_text[:80]}...\")\n",
        "        print(f\"   Group: {request.group}\")\n",
        "        \n",
        "        print(f\"\\nüìä Analysis Result (ID: {result.id})\")\n",
        "        print(f\"   Score: {result.score}\")\n",
        "        print(f\"   Categories: {result.categories}\")\n",
        "        print(f\"   Summary: {result.summary[:200]}...\")\n",
        "        print(f\"   Model: {result.model_version}\")\n",
        "        print(f\"   Validation: {result.validation_status}\")\n",
        "        \n",
        "        if result.llm_trace:\n",
        "            print(f\"\\nüîç LLM Trace:\")\n",
        "            print(f\"   Keys: {list(result.llm_trace.keys())}\")\n",
        "            if \"tools_called\" in result.llm_trace:\n",
        "                print(f\"   Tools called: {result.llm_trace['tools_called']}\")\n",
        "                \n",
        "    except PermissionError as e:\n",
        "        print(f\"‚ùå Permission denied: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test RBAC/ABAC (Different Users)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all available mock users\n",
        "print(\"Available mock users:\")\n",
        "for user_key, user in MOCK_USERS.items():\n",
        "    print(f\"\\n  {user_key}:\")\n",
        "    print(f\"    Role: {user.role}\")\n",
        "    print(f\"    Permissions: {[p.value for p in user.permissions]}\")\n",
        "    print(f\"    Groups: {user.groups}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test RBAC - viewer should NOT be able to analyze\n",
        "viewer = get_current_user(\"viewer_a\")\n",
        "print(f\"Testing as VIEWER: {viewer.user_id}\")\n",
        "print(f\"  Permissions: {[p.value for p in viewer.permissions]}\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor = Processor(session, user=viewer)\n",
        "    \n",
        "    try:\n",
        "        request, result = processor.process_request(RequestCreate(\n",
        "            input_text=\"Test transaction\",\n",
        "        ))\n",
        "        print(\"‚ùå Should have failed! Viewer shouldn't be able to analyze.\")\n",
        "    except PermissionError as e:\n",
        "        print(f\"‚úÖ Correctly blocked: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ABAC - analysts only see their group's data\n",
        "analyst_a = get_current_user(\"analyst_a\")\n",
        "analyst_b = get_current_user(\"analyst_b\")\n",
        "\n",
        "print(f\"Analyst A groups: {analyst_a.groups}\")\n",
        "print(f\"Analyst B groups: {analyst_b.groups}\")\n",
        "\n",
        "with get_session() as session:\n",
        "    processor_a = Processor(session, user=analyst_a)\n",
        "    results_a = processor_a.get_recent_results(limit=10)\n",
        "    \n",
        "    processor_b = Processor(session, user=analyst_b)\n",
        "    results_b = processor_b.get_recent_results(limit=10)\n",
        "    \n",
        "    print(f\"\\nAnalyst A sees {len(results_a)} results\")\n",
        "    print(f\"Analyst B sees {len(results_b)} results\")\n",
        "    \n",
        "    if results_a:\n",
        "        print(f\"Analyst A result groups: {set(r.group for r in results_a)}\")\n",
        "    if results_b:\n",
        "        print(f\"Analyst B result groups: {set(r.group for r in results_b)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Validation / Guardrails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test validation functions directly\n",
        "from app.services.llm_service import LLMResponse\n",
        "\n",
        "# Create mock LLM response WITH potential PII leakage\n",
        "mock_response = LLMResponse(\n",
        "    score=75,\n",
        "    categories=[\"suspicious\", \"high_value\"],\n",
        "    summary=\"This transaction shows signs of potential money laundering. The sender's SSN is 123-45-6789.\",\n",
        "    reasoning=\"Based on the pattern analysis...\",\n",
        ")\n",
        "\n",
        "original_input = \"Wire transfer from John Smith\"\n",
        "\n",
        "status, details = run_all_validations(mock_response, original_input)\n",
        "\n",
        "print(f\"Validation status: {status}\")\n",
        "print(f\"Details: {details}\")\n",
        "\n",
        "# If you implemented PII detection, this should catch the SSN\n",
        "if details and \"PII\" in str(details):\n",
        "    print(\"\\n‚úÖ PII detection working!\")\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è PII detection not implemented yet (Phase 4)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Scenarios (E2E)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test scenarios for the KYC/AML case\n",
        "TEST_SCENARIOS = [\n",
        "    {\n",
        "        \"name\": \"Clean Transaction\",\n",
        "        \"input\": \"Payment for consulting services from ABC Corp, $2,500\",\n",
        "        \"expected_risk\": \"LOW\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Near Threshold (Structuring)\",\n",
        "        \"input\": \"Cash deposit $9,500 - monthly savings\",\n",
        "        \"expected_risk\": \"MEDIUM\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Sanctions Match\",\n",
        "        \"input\": \"Wire transfer from Ahmed Ivanov for equipment purchase, $15,000\",\n",
        "        \"expected_risk\": \"CRITICAL\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"PEP Transaction\",\n",
        "        \"input\": \"Donation from Elena Volkova for charity event, $50,000\",\n",
        "        \"expected_risk\": \"HIGH\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(TEST_SCENARIOS)} test scenarios:\")\n",
        "for i, scenario in enumerate(TEST_SCENARIOS, 1):\n",
        "    print(f\"  {i}. {scenario['name']} - Expected: {scenario['expected_risk']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all test scenarios\n",
        "def run_test_scenario(scenario: dict, user: UserProfile):\n",
        "    \"\"\"Run a single test scenario and return results.\"\"\"\n",
        "    with get_session() as session:\n",
        "        processor = Processor(session, user=user)\n",
        "        \n",
        "        request_data = RequestCreate(\n",
        "            input_text=scenario[\"input\"],\n",
        "            context=f\"Test: {scenario['name']}\",\n",
        "        )\n",
        "        \n",
        "        request, result = processor.process_request(request_data)\n",
        "        \n",
        "        return {\n",
        "            \"name\": scenario[\"name\"],\n",
        "            \"expected\": scenario[\"expected_risk\"],\n",
        "            \"actual_score\": result.score,\n",
        "            \"categories\": result.categories,\n",
        "            \"validation\": result.validation_status,\n",
        "            \"summary\": result.summary[:100] + \"...\",\n",
        "        }\n",
        "\n",
        "# Run scenarios\n",
        "user = get_current_user(\"analyst_a\")\n",
        "\n",
        "print(\"Running test scenarios...\\n\")\n",
        "for scenario in TEST_SCENARIOS:\n",
        "    try:\n",
        "        result = run_test_scenario(scenario, user)\n",
        "        \n",
        "        # Determine risk level from score\n",
        "        score = result[\"actual_score\"]\n",
        "        if score <= 25:\n",
        "            actual_level = \"LOW\"\n",
        "        elif score <= 50:\n",
        "            actual_level = \"MEDIUM\"\n",
        "        elif score <= 75:\n",
        "            actual_level = \"HIGH\"\n",
        "        else:\n",
        "            actual_level = \"CRITICAL\"\n",
        "        \n",
        "        match = \"‚úÖ\" if actual_level == result[\"expected\"] else \"‚ö†Ô∏è\"\n",
        "        \n",
        "        print(f\"{match} {result['name']}\")\n",
        "        print(f\"   Expected: {result['expected']}, Got: {actual_level} (score: {score})\")\n",
        "        print(f\"   Categories: {result['categories']}\")\n",
        "        print(f\"   Validation: {result['validation']}\")\n",
        "        print()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {scenario['name']}: {e}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Debug Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: View recent results from DB\n",
        "from sqlmodel import select\n",
        "\n",
        "with get_session() as session:\n",
        "    stmt = select(AnalysisResult).order_by(AnalysisResult.created_at.desc()).limit(5)\n",
        "    results = session.exec(stmt).all()\n",
        "    \n",
        "    print(f\"Last {len(results)} analysis results:\\n\")\n",
        "    for r in results:\n",
        "        print(f\"ID: {r.id} | Score: {r.score} | Status: {r.validation_status}\")\n",
        "        print(f\"   Categories: {r.categories}\")\n",
        "        print(f\"   Created: {r.created_at}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: View LLM trace for a specific result\n",
        "import json\n",
        "\n",
        "result_id = 1  # Change this to inspect different results\n",
        "\n",
        "with get_session() as session:\n",
        "    result = session.get(AnalysisResult, result_id)\n",
        "    \n",
        "    if result and result.llm_trace:\n",
        "        print(f\"LLM Trace for result {result_id}:\")\n",
        "        print(json.dumps(result.llm_trace, indent=2, default=str))\n",
        "    else:\n",
        "        print(f\"No trace found for result {result_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: Clear all test data (use carefully!)\n",
        "# Uncomment to run\n",
        "\n",
        "# from sqlmodel import text\n",
        "# with get_session() as session:\n",
        "#     session.exec(text(\"DELETE FROM analysis_results\"))\n",
        "#     session.exec(text(\"DELETE FROM requests\"))\n",
        "#     print(\"‚úÖ All test data cleared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Quick Reference\n",
        "\n",
        "### Before Interview:\n",
        "1. `docker compose up -d postgres` - Start database\n",
        "2. Check LLM provider (Ollama running OR API key set)\n",
        "3. Run Setup cells (1.1 - 1.3)\n",
        "\n",
        "### During Interview - Quick Validation:\n",
        "| Phase | Test Section |\n",
        "|-------|-------------|\n",
        "| Phase 1 (Models) | Section 2 |\n",
        "| Phase 2 (Tools) | Section 3 |\n",
        "| Phase 3 (Prompts) | Sections 4-5 |\n",
        "| Phase 4 (Validation) | Section 7 |\n",
        "| Phase 5 (Processor) | Section 5 |\n",
        "| Phase 6 (UI) | Streamlit browser |\n",
        "\n",
        "### Hotkeys:\n",
        "- `Shift+Enter` - Run cell and move to next\n",
        "- `Ctrl+Enter` - Run cell and stay\n",
        "- `Esc + A` - Insert cell above\n",
        "- `Esc + B` - Insert cell below\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
